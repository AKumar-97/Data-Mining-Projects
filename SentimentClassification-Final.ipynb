{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c98c0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sl\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121cb987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Eat at Fioris, they said.  Youll like it, they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>I just don't understand the appeal.  I've trie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This is my go to place for a really good beef ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>Not impressed. When I ordered the Oyako bowl, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>This is the first time ever I wrote a bad revi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>I don't really mind dive places because there ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a frequent dining spot for me. There a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>I'm pretty happy with my purchase today. I fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>This is by far one of my favorite spots!! \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>Do not use this dry cleaner. The staff/owner s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1\n",
       "0 -1  Eat at Fioris, they said.  Youll like it, they...\n",
       "1 -1  I just don't understand the appeal.  I've trie...\n",
       "2  1  This is my go to place for a really good beef ...\n",
       "3 -1  Not impressed. When I ordered the Oyako bowl, ...\n",
       "4 -1  This is the first time ever I wrote a bad revi...\n",
       "5 -1  I don't really mind dive places because there ...\n",
       "6  1  This is a frequent dining spot for me. There a...\n",
       "7  1  I'm pretty happy with my purchase today. I fou...\n",
       "8  1  This is by far one of my favorite spots!! \\n\\n...\n",
       "9 -1  Do not use this dry cleaner. The staff/owner s..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First Step is to import and read the the training and testing dataset\n",
    "\n",
    "data_train_df = pd.read_csv('E:/Sem-3/Data Mining/Hw1 Data/1661892619_92027_train_file.csv',header=None)\n",
    "data_test_df = pd.read_csv('E:/Sem-3/Data Mining/Hw1 Data/1661892619_9579706_test_file.csv',header=None)\n",
    "data_train_df.head(10)\n",
    "#data_test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cf04a84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "#adding column names to the training dataset\n",
    "data_train_df.columns = ['Sentiment', 'Review']\n",
    "data_test_df.columns = ['Review']\n",
    "#data_train_df.head(10)\n",
    "#data_test_df.head(10)\n",
    "data_train_Review_df = data_train_df['Review'].str.lower()\n",
    "data_train_Sentiment_df = data_train_df['Sentiment']\n",
    "data_test_df = data_test_df['Review'].str.lower()\n",
    "print(type(data_train_Review_df))\n",
    "print(len(data_train_Sentiment_df))\n",
    "print(len(data_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8646edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we try to tokenize the data\n",
    "def tokenizeData(data):\n",
    "    data = word_tokenize(data)\n",
    "    #toktokTokenize = ToktokTokenizer()\n",
    "    #data = toktokTokenize.tokenize(data)\n",
    "#     tweet_tokenizer = TweetTokenizer()\n",
    "#     data = tweet_tokenizer.tokenize(str(data))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ea6049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "#calling the tokenize function\n",
    "data_train_Review_df = list(map(tokenizeData, data_train_Review_df))\n",
    "print(len(data_train_Review_df))\n",
    "data_test_df = list(map(tokenizeData, data_test_df))\n",
    "print(len(data_test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29e5b7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to clean the data\n",
    "# first step is to look to remove any special characters\n",
    "def remSpecialChar(data):\n",
    "    #removing special character\n",
    "    data = re.sub(\"[^a-zA-z0-9\\s]\", \"\", data)\n",
    "    #removing brackets\n",
    "    data = re.sub(\"\\[[^]]*\\]\", \"\", data)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#removing stop words\n",
    "def remStop(data):\n",
    "    stop_words = stopwords.words('english')\n",
    "    # we try to remove certain negative words like 'not' from the list of stopwords\n",
    "    stop_words.remove(\"not\")\n",
    "    stop_words.remove(\"no\")\n",
    "    stop_words.remove(\"don't\")\n",
    "    data = [textWord for textWord in data if textWord not in (stop_words)]\n",
    "    data = \" \".join(data)\n",
    "    return data\n",
    "\n",
    "def dataClean(data):\n",
    "    data = remStop(data)\n",
    "    data = remSpecialChar(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81338da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "#Now we call the cleaning functions defined above\n",
    "data_train_Review_Clean = list(map(dataClean, data_train_Review_df))\n",
    "data_test_Clean = list(map(dataClean, data_test_df))\n",
    "print(len(data_train_Review_Clean))\n",
    "print(len(data_test_Clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af95a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemm = LancasterStemmer()\n",
    "# #stemm = SnowballStemmer(language='english')\n",
    "# def stemmer(data):\n",
    "#     data = \" \".join([stemm.stem(singleWord) for singleWord in data.split()])\n",
    "#     return data\n",
    "\n",
    "#the lemm function\n",
    "lemm = WordNetLemmatizer()\n",
    "def lemmatizer01(data):\n",
    "    data = \" \".join([lemm.lemmatize(singleWord) for singleWord in data.split()])\n",
    "    return data\n",
    "\n",
    "# print(data_train_Review_Processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "fd69b5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "# #calling the stemm function\n",
    "# data_train_Review_Processed = []\n",
    "# data_test_Processed = []\n",
    "# #Stemming training data\n",
    "# data_train_Review_Processed = list(map(stemmer,data_train_Review_Clean))\n",
    "# print(len(data_train_Review_Processed))\n",
    "# #Stemming testing data\n",
    "# data_test_Processed = list(map(stemmer,data_test_Clean))\n",
    "# print(len(data_test_Processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1c2d11ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n",
      "18000\n"
     ]
    }
   ],
   "source": [
    "# calling the lemm function\n",
    "data_train_Review_Processed = []\n",
    "data_test_Processed = []\n",
    "#lemmatizing the training data\n",
    "data_train_Review_Processed = list(map(lemmatizer01,data_train_Review_Clean))\n",
    "print(len(data_train_Review_Processed))\n",
    "#lemmatizing the testing data\n",
    "data_test_Processed = list(map(lemmatizer01,data_test_Clean))\n",
    "print(len(data_test_Processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "131cc02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 28281)\n",
      "(18000, 28281)\n"
     ]
    }
   ],
   "source": [
    "# # The next step is to vectorize the data and extract the features\n",
    "# # we try to implement countVectorizer()\n",
    "# #\n",
    "# count_vector = CountVectorizer()\n",
    "# # fitting and transforming training data\n",
    "# data_train_Review_matrix = count_vector.fit_transform(data_train_Review_Processed)\n",
    "# # transforming test data\n",
    "# data_test_matrix = count_vector.transform(data_test_Processed)\n",
    "# print(data_train_Review_matrix.shape)\n",
    "# print(data_test_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "145297c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 978)\n",
      "(18000, 978)\n"
     ]
    }
   ],
   "source": [
    "# We try to vectorize the data using tfidf vectorizer\n",
    "#min_df=0.05, max_df=0.7\n",
    "#min_df==0.01, max_df =0.96\n",
    "#norm = \"l2\"\n",
    "tfidf_vector = TfidfVectorizer(min_df=0.01,max_df=0.96)\n",
    "#fitting and transforming training data\n",
    "data_train_Review_matrix_tf = tfidf_vector.fit_transform(data_train_Review_Processed)\n",
    "#transforming test data\n",
    "data_test_matrix_tf = tfidf_vector.transform(data_test_Processed)\n",
    "print(data_train_Review_matrix_tf.shape)\n",
    "print(data_test_matrix_tf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "66e34ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function to calculate cosine simillarity\n",
    "# this is to find simillarity between train and test vectorized data\n",
    "def cosSim(train_matrix,test_matrix):\n",
    "    train_matrix_transpose = np.transpose(train_matrix)\n",
    "    dotProd = np.dot(test_matrix, train_matrix_transpose)\n",
    "    y_axisPre = test_matrix.toarray()\n",
    "    x_axisPre = train_matrix_transpose.toarray()\n",
    "    #now we try to use the formula for simillarity\n",
    "    #Similarity = (A.B) / (||A||.||B||)\n",
    "    y_axis = math.sqrt(sum([np.dot(j,j) for j in y_axisPre]))\n",
    "    x_axis = math.sqrt(sum([np.dot(k,k) for k in x_axisPre]))\n",
    "    numerator = dotProd.toarray()\n",
    "    simillarityValue = [numVal/(y_axis*x_axis)for numVal in numerator]\n",
    "    return simillarityValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79d64677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define function to sort the nearest neighbours/values\n",
    "def KNN_findNeighbour(simValueVector, k):\n",
    "    return np.argsort(-simValueVector)[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed18c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define a function to take the nearest neighbours and make predictions on the same\n",
    "def predict(k_nearest_neighbours, labelSentiments):\n",
    "    posLabelCount = 0\n",
    "    negLabelCount = 0\n",
    "    for neighbour in k_nearest_neighbours:\n",
    "        if int(labelSentiments[neighbour]) == 1:\n",
    "            #its closer to positive sentiment\n",
    "            posLabelCount = posLabelCount + 1\n",
    "        else:\n",
    "            negLabelCount = negLabelCount + 1\n",
    "    if (negLabelCount > posLabelCount):\n",
    "        # on aggr the datapoint is closer to positive sentiment\n",
    "        return -1\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "127eb9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Approach at prediction\n",
    "k = 1001\n",
    "#k=151--lemm#k=101--lemmatizer#k=101--stemming#30 #3 #20\n",
    "#k=155--lemm--norml2\n",
    "#k=999--lemm--minmax\n",
    "#k=1001--lemm--minmax\n",
    "# Making use of matrix formed by using CountVectorizer for prediction\n",
    "# cosine_simValue = cosSim(data_train_Review_matrix,data_test_matrix)\n",
    "\n",
    "# Making use of matrix formed by using TFIDFVectorizer for prediction\n",
    "cosine_simValue = cosSim(data_train_Review_matrix_tf,data_test_matrix_tf)\n",
    "data_test_Sentiment = list()\n",
    "\n",
    "for simValue in cosine_simValue:\n",
    "    neighbour = KNN_findNeighbour(simValue, k)\n",
    "    #predicting sentiment\n",
    "    prediction = predict(neighbour, data_train_Sentiment_df.tolist())\n",
    "    if prediction == 1:\n",
    "        data_test_Sentiment.append('1')\n",
    "    else:\n",
    "        data_test_Sentiment.append('-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb0a3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we write the Sentiments predicted above to a file\n",
    "analysisResult = open(\"ResultFile209.dat\", \"w\")\n",
    "analysisResult.writelines(\"%s\\n\" % sentiment for sentiment in data_test_Sentiment)\n",
    "analysisResult.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef740288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
